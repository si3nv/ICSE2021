#!/usr/bin/env python3

"""

"""
import argparse
from collections import defaultdict, OrderedDict
import csv
import logging
import math
import os
import sys
from typing import Optional

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.python.framework import ops
from tensorflow.python.keras import backend as K
from tensorflow.python.ops import math_ops

# with eager execution enabled (default), model quickly runs out of RAM
tf.compat.v1.disable_eager_execution()
import utils


DEFAULT_BATCH_SIZE = 32
_MODE_FNAMES = {
    'dev': 'global_dev_imports.csv',
    'proj': 'global_proj_imports.csv',
}


def get_languages(path, detect_suffix='_dev_commit_counts.csv'):
    for fname in os.listdir(path):
        if (fname.endswith(detect_suffix)
                and os.path.isfile(os.path.join(path, fname))):
            lang = fname[:-len(detect_suffix)]
            if (os.path.isfile(os.path.join(path, lang + '_vocab.csv'))
                    and os.path.isfile(os.path.join(
                        path, lang + '_namespace_counts_by_dev.csv'))):
                yield lang


def get_global_namespace(lang, namespace):
    if lang == 'ipy':  # WoC parses only Python imports in Jupyter notebooks
        lang = 'PY'
    return lang + ':' + namespace


def build_global_data(path, ns_whitelist_path='ns_whitelist', min_year=2008,
                bot_threshold=0.005, reserved_tokens=30):
    # type: (str, str, int, float, int) -> None
    """
    Args:
        path (str): path to search for data files
        ns_whitelist_path (str): path to look for namespace whitelists.
            These files are generated by get_ns_whitelists.py
        min_year (int): min year to consider in dev profiles
        bot_threshold (float): share of top-committing developers to skip
            Most of these commits come from automated activity and create
            harmful connections between projects in different domains
        reserved_tokens (int): number of tokens to reserve for special tokens.
            this is left for future use to represent out of vocabulary tokens.
    Returns:
        None
    """
    languages = list(get_languages(path))
    if not languages:
        sys.stderr.write('\nNo data files found in the specified path\n')
        return

    # build global vocabulary
    vocabulary = OrderedDict()  # vocabulary[token]: id
    # reserve some space for special cases
    for i in range(reserved_tokens):
        vocabulary['[RES%d]' % i] = i

    # associate numeric ids to each vocab entity
    for lang in languages:
        real_lang = 'PY' if lang == 'ipy' else lang
        whitelist_fname = os.path.join(
            ns_whitelist_path, real_lang+'_whitelist.csv')
        if os.path.isfile(whitelist_fname):
            with open(whitelist_fname) as fh:
                ns_whitelist = set(get_global_namespace(lang, ns)
                                   for ns in fh.read().split('\n'))
        else:
            sys.stderr.write("Whitelist not found for %s\n" % lang)
            ns_whitelist = None

        with open(os.path.join(path, lang + '_vocab.csv')) as fh:
            for line in fh:
                namespace = get_global_namespace(lang, line.strip())
                if ns_whitelist and namespace not in ns_whitelist:
                    continue
                if namespace in vocabulary:
                    # e.g., ipy namespace already processed by PY
                    continue
                vocabulary[namespace] = len(vocabulary)
    with open('global_vocab.csv', 'wb') as fh:
        fh.write('\n'.join(vocabulary.keys()))

    # global dev commit count to filter out bots
    dev_commit_count = defaultdict(int)  # dev_commit_count[email] = total_count
    for lang in languages:
        with open(os.path.join(path, lang + '_dev_commit_counts.csv'), 'rU') as fh:
            reader = csv.reader(fh)
            for row in reader:
                if len(row) != 2:
                    # at least one record is missing count
                    # TODO: debug preprocess.py ['cristiano@friendfund.com ']
                    continue
                email, str_count = row
                dev_commit_count[email.strip()] += int(str_count)
    dev_commit_count = pd.Series(dev_commit_count).sort_values(ascending=False)
    alleged_bots_num = int(len(dev_commit_count) * bot_threshold)
    alleged_bots = set(dev_commit_count.index[:alleged_bots_num])

    # create training data
    # using set because different languages can contribute same tokens,
    # e.g. ipy will be contributing to PY: tokens
    dev_profiles = defaultdict(lambda: defaultdict(set))
    csv.field_size_limit(2147483647)  # DANGER ZONE, but won't read otherwise
    for lang in languages:
        with open(os.path.join(path, lang + '_dev_year_imports.csv')) as fh:
            for email, year, csv_namespaces in csv.reader(fh):
                year = int(year)
                if year < min_year:
                    continue
                email = email.strip()
                if email in alleged_bots:
                    continue
                namespaces = (get_global_namespace(lang, i)
                              for i in csv_namespaces.split(','))
                ids = [vocabulary[ns] for ns in namespaces if ns in vocabulary]
                if not ids:
                    # sometimes developer year will contain only
                    # out-of-vocabulary namespaces, which will result in empty
                    # .update(). The key will exist in the dictionary then, but
                    # the set will be empty - better skip such cases
                    continue
                dev_profiles[email][year].update(ids)

    # first model, all years combined:
    with open('global_dev_imports.csv', 'wb') as fh:
        writer = csv.writer(fh)
        for dev, dev_year_imports in dev_profiles.items():
            all_imoprts = set().union(*dev_year_imports.values())
            writer.writerow([dev] + list(all_imoprts))

    # input only, separated by years:
    # Will be used for input->input autoencoder
    with open('global_dev_year_imports.csv', 'wb') as fh:
        writer = csv.writer(fh)
        for dev, dev_year_imports in dev_profiles.items():
            for year, import_set in dev_year_imports.items():
                writer.writerow([dev, year] + list(import_set))

    # third model and performance evaluation:
    # predict next year from this year data
    # NOTE: since we're logging only consecutive year pairs, there is
    # substantially less data than in two previous files
    with open('global_dev_year_imports_and_next.csv', 'wb') as fh:
        writer = csv.writer(fh)
        for dev, dev_year_imports in dev_profiles.items():
            years = sorted(dev_year_imports.keys())
            for this_year, next_year in zip(years, years[1:]):
                if next_year - this_year != 1:
                    continue
                this_year_imports = ','.join(
                    str(i) for i in dev_year_imports[this_year])
                next_year_imports = ','.join(
                    str(i) for i in dev_year_imports[next_year])
                writer.writerow((dev, year, this_year_imports, next_year_imports))


def build_project_vocab(path, min_year=2008):
    languages = list(get_languages(path))

    with open('global_vocab.csv') as fh:
        vocabulary = {line.strip(): i for i, line in enumerate(fh)}

    # create training data
    # using set because different languages can contribute same tokens,
    # e.g. ipy will be contributing to PY: tokens
    proj_profiles = defaultdict(lambda: defaultdict(set))
    csv.field_size_limit(2147483647)  # DANGER ZONE, but won't read otherwise
    for lang in languages:
        with open(os.path.join(path, lang + '_projects_year_imports.csv')) as fh:
            for proj, year, csv_namespaces in csv.reader(fh):
                year = int(year)
                if year < min_year:
                    continue
                proj = proj.strip()
                namespaces = (get_global_namespace(lang, i)
                              for i in csv_namespaces.split(','))
                ids = [vocabulary[ns] for ns in namespaces if ns in vocabulary]
                if not ids:
                    continue
                proj_profiles[proj][year].update(ids)

    with open('global_proj_imports.csv', 'wb') as fh:
        writer = csv.writer(fh)
        for proj, proj_year_imports in proj_profiles.items():
            all_imoprts = set().union(*proj_year_imports.values())
            writer.writerow([proj] + list(all_imoprts))

    with open('global_proj_year_imports.csv', 'wb') as fh:
        writer = csv.writer(fh)
        for proj, proj_year_imports in proj_profiles.items():
            for year, import_set in proj_year_imports.items():
                writer.writerow([proj, year] + list(import_set))


class dev2vecSequence(tf.keras.utils.Sequence):
    def __init__(self, input_data, input_offsets,
                 output_data=None, output_offsets=None,
                 vocab_size=None, batch_size=DEFAULT_BATCH_SIZE):
        if not vocab_size:
            raise ValueError('Vocabulary size is expected')
        self.input_data = input_data
        self.input_offsets = input_offsets
        self.output_data = output_data
        self.output_offsets = output_offsets
        self.batch_size = batch_size
        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(vocab_size)

    def __len__(self):
        return math.floor(len(self.input_offsets) / self.batch_size)

    def __getitem__(self, idx):
        # mode='binary' by default
        batch_input = self.tokenizer.sequences_to_matrix(
            [list(self.input_data[start:end])
             for start, end in self.input_offsets[
                               idx*self.batch_size:(idx+1)*self.batch_size]])
        if self.output_data is not None:
            batch_output = self.tokenizer.sequences_to_matrix(
                [list(self.output_data[start:end])
                 for start, end in self.output_offsets[
                                   idx*self.batch_size:(idx+1)*self.batch_size]])
        else:
            batch_output = batch_input
        return batch_input, batch_output


def get_ns_whitelist(language, ns_whitelist_path='ns_whitelist',
                     data_path='data'):
    # type: (str, str, str) -> set
    whitelist_fname = os.path.join(
        ns_whitelist_path, language + '_whitelist.csv')
    if language == 'global' and not os.path.isfile(whitelist_fname):
        logging.info('Global ns whitelist does not exist, building it')
        with open(whitelist_fname, 'w') as whitelist_fh:
            for _lang in get_languages(data_path):
                _whitelist_fname = os.path.join(
                    ns_whitelist_path, _lang + '_whitelist.csv')
                if not os.path.isfile(_whitelist_fname):
                    logging.warning('Vocab for %s is not found', _lang)
                    continue
                with open(_whitelist_fname) as _whitelist_fh:
                    for line in _whitelist_fh:
                        whitelist_fh.write(_lang + ':' + line)
    with open(whitelist_fname) as whitelist_fh:
        return {ns.strip() for ns in whitelist_fh}


def build_training_data(language, ns_whitelist_path='ns_whitelist',
                        data_path='data', min_use_threshold=100):
    # type: (str, str, str, int, int) -> None
    """
    iterate through commits, collecting X projects, their developers and
        library adoption dates for both

    test_val_threshold is the number of records to store in test and validation

    Projects: 6.5M Python, 7.5M JS
    Dev: 4.3M PY, 3.4M JS

    """
    final_vocab_fname = os.path.join(data_path, language + '_final_vocab.csv')
    if os.path.isfile(final_vocab_fname):
        return

    ns_use_counts_fname = os.path.join(
        data_path, language + '_namespace_counts_by_projects.csv')
    whitelist = get_ns_whitelist(language, ns_whitelist_path, data_path)
    # filter out low usage ones
    with open(ns_use_counts_fname) as ns_use_counts_fh:
        reader = csv.reader(ns_use_counts_fh)
        # 22k entries PY, 33k JS
        vocab = {i: namespace for i, (namespace, count) in enumerate(reader)
                 if int(count) >= min_use_threshold}

    with open(final_vocab_fname, 'w') as final_vocab_fh:
        final_vocab_fh.writelines(vocab[i]+'\n' for i in range(len(vocab)))

    # write train data
    for mode in ('dev', 'projects'):
        input_fname = ''


def get_weighted_loss_fn(label_weights):
    label_weights = math_ops.cast(label_weights, tf.float32)

    def loss_fn(y_true, y_pred):
        y_pred = ops.convert_to_tensor(y_pred)
        y_true = math_ops.cast(y_true, y_pred.dtype)
        return K.mean(
            label_weights *
            K.binary_crossentropy(y_true, y_pred, from_logits=False), axis=-1)
    return loss_fn


def get_nn_model(vocab_size, embed_size,
                 normalize=False, batch_size=DEFAULT_BATCH_SIZE):
    # we need to apply normalization along different axes, this is why `if`
    # instead of parametrization
    if normalize:
        # https://arxiv.org/pdf/1607.06450.pdf
        sys.stderr.write('Training WITH weights normalization\n')
        layers = [
            tf.keras.layers.Dense(
                embed_size, activation='linear', batch_size=batch_size,
                kernel_constraint=tf.keras.constraints.UnitNorm(axis=1),
                use_bias=use_bias),
            tf.keras.layers.Dense(
                vocab_size, activation='sigmoid', use_bias=use_bias,
                kernel_constraint=tf.keras.constraints.UnitNorm(axis=0))
        ]
    else:
        sys.stderr.write('Training WITHOUT weights normalization\n')
        layers = [
            tf.keras.layers.Dense(
                embed_size, activation='linear', batch_size=batch_size,
                use_bias=use_bias),
            tf.keras.layers.Dense(
                vocab_size, activation='sigmoid', use_bias=use_bias)
        ]
    return tf.keras.Sequential(layers)


def train(language, embed_size, mode='proj',
          model_save_path=utils.DEFAULT_MODEL_PATH,
          data_path=utils.DEFAULT_DATA_PATH,
          batch_size=DEFAULT_BATCH_SIZE,
          weighted=False, normalize=False, use_bias=True):
    # type: (str, int, str, str, str, int, bool, bool, bool) -> None
    assert os.path.isdir(data_path), 'Data path %s doesn\'t exist'
    assert language == 'global' or language in utils.ECOSYSTEMS, \
        'Unknown programming language'
    assert mode in _MODE_FNAMES, (
            "Invalid mode '%s'. Should be one of %s" % (
                mode, ','.join(_MODE_FNAMES)))

    model_save_fname = os.path.join(model_save_path, '%s_%s_%d%s%s%s.model' % (
        language, mode, embed_size, '_norm' if normalize else '',
        '_no_bias' if not use_bias else '',
        '_weighted' if weighted else '',
    ))
    sys.stderr.write('The model will be saved to: %s\n' % model_save_fname)

    if not os.path.isdir(model_save_path):
        os.mkdir(model_save_path)

    vocab_path = os.path.join(data_path, language + '_vocab.csv')
    if language == 'global' and not os.path.isfile(vocab_path):
        # TODO: implement
        raise NotImplementedError
        # build_global_data(language)

    sys.stderr.write('Reading vocabulary..\n')
    idx2namespace, namespace2idx = utils.read_vocab(vocab_path)
    vocab_size = len(idx2namespace)

    sys.stderr.write('Reading dataset..\n')
    csv.field_size_limit(2147483647)  # DANGER ZONE, but won't read otherwise
    imports_prefix = '%s_%s_imports_' % (language, mode)

    input_data_train, input_offsets_train = utils.read_dev(
        os.path.join(data_path, imports_prefix + 'train.csv'), namespace2idx)
    input_data_val, input_offsets_val = utils.read_dev(
        os.path.join(data_path, imports_prefix + 'val.csv'), namespace2idx)

    dataset_train = dev2vecSequence(
        input_data_train, input_offsets_train,
        vocab_size=vocab_size, batch_size=batch_size)
    dataset_val = dev2vecSequence(
        input_data_val, input_offsets_val,
        vocab_size=vocab_size, batch_size=batch_size)

    model = get_nn_model(
        vocab_size, embed_size, normalize=False, batch_size=DEFAULT_BATCH_SIZE)

    loss_fn = 'binary_crossentropy'
    if weighted:
        counts_fname = os.path.join(
            data_path, '%s_namespace_counts_by_%s.csv' % (
                language, 'projects' if mode == 'proj' else mode))
        counts = df = pd.read_csv(
            counts_fname, header=None, index_col=0, squeeze=True)
        counts = counts[
            [idx2namespace[idx] for idx in range(len(idx2namespace))]]
        label_weights = counts / counts.median()
        loss_fn = get_weighted_loss_fn(label_weights)

    model.compile(optimizer='adam', loss=loss_fn)
    # in most cases, model starts to overfit after less than one epoch
    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)

    sys.stderr.write('Training..\n')
    model.fit(dataset_train, epochs=2, callbacks=[callback],
              validation_data=dataset_val)

    sys.stderr.write('Saving the model..\n')
    model.save(model_save_fname)


def train_lstm(lang, mode, vocab_path,
               model_save_path=utils.DEFAULT_MODEL_PATH,
               data_path=utils.DEFAULT_DATA_PATH,
               separator=utils.DEFAULT_LSTM_SEPARATOR):
    assert mode in ('proj', 'dev')
    idx2namespace, namespace2idx = utils.read_vocab(vocab_path, lstm=True)
    model = utils.get_lstm_model(len(namespace2idx))
    train_data_path = os.path.join(
        data_path, '%s_%s_lstm_train.csv' % (lang, mode))
    val_data_path = os.path.join(
        data_path, '%s_%s_lstm_val.csv' % (lang, mode))
    train_data = utils.LSTMSequence(train_data_path)
    validation_data = utils.LSTMSequence(train_data_path)

    model_fname = os.path.join(
        model_save_path, '%s_%s_lstm.model' % (lang, mode))
    checkpoint_fname = model_fname + '.chkpt'
    cp_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_fname, save_weights_only=True)
    model.fit(train_data, epochs=2, callbacks=[cp_callback],
              validation_data=validation_data)
    model.save(model_fname)


def train_lsi(lang, mode, vocab_path,
              model_save_path=utils.DEFAULT_MODEL_PATH,
              data_path=utils.DEFAULT_DATA_PATH):
    assert mode in ('proj', 'dev')
    from gensim.models import LsiModel
    # LsiModel accepts corpus as a list of 2-tuples (token id, token count)
    imports_prefix = '%s_%s_imports_' % (lang, mode)
    idx2namespace, namespace2idx = utils.read_vocab(vocab_path)
    input_data_train, input_offsets_train = utils.read_dev(
        os.path.join(data_path, imports_prefix + 'train.csv'), namespace2idx)

    corpus = [[(idx, 1) for idx in input_data_train[start:finish]]
              for start, finish in input_offsets_train]
    # takes ~40 minutes on tdg
    model = LsiModel(corpus, id2word=idx2namespace, num_topics=100)
    model_fname = os.path.join(
        model_save_path, '%s_%s_LSI.model' % (lang, mode))


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Train autoencoder model with the specified dimension')
    parser.add_argument('language', choices=['global'] + list(utils.ECOSYSTEMS),
                        help='Language to use')
    parser.add_argument('embed_size', type=int, default=100, nargs='?',
                        help='Embeddings size')
    parser.add_argument('-b', '--batch-size', default=DEFAULT_BATCH_SIZE,
                        type=int, help='Batch size')
    parser.add_argument('--save-path', default='models', type=str,
                        help='Directory to save trained models to')
    parser.add_argument('--normalize', action='store_true',
                        help='Whether to normalize weights. See '
                             'https://arxiv.org/abs/1602.07868 and '
                             'https://arxiv.org/pdf/1607.06450.pdf for details')
    parser.add_argument('--disable-bias', action='store_true',
                        help='Whether to bias')
    parser.add_argument('--weighted', action='store_true',
                        help='Decrease importance of common entries')
    parser.add_argument('--mode', choices=_MODE_FNAMES.keys(),
                        default='dev', help='''Type of the model to train.
    'dev' will train autoencoder to reconstruct full developer profile
    'dev-year' will train on one year slices from developer profiles
    'dev-year-next' will train to predict next year of developers' history
    ''')
    args = parser.parse_args()

    use_bias = not args.disable_bias
    train(args.language, args.embed_size, args.mode, batch_size=args.batch_size,
          model_save_path=args.save_path, normalize=args.normalize,
          use_bias=use_bias, weighted=args.weighted)
